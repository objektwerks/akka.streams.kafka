kafka {
  topic = "akka-streams-kafka"
  partitions = 100
}
akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "INFO"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    debug.receive = true
  }

  kafka.producer {
    bootstrap.servers = "localhost:6001"
    client.id = "akka-streams-kafka-producer-id"
    acks = "all"
    retries = 1
    batch.size = 16384
    linger.ms = 1
    buffer.memory = 33554432
    key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
    value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
    parallelism = 100
    close-timeout = 60s
    use-dispatcher = "akka.kafka.default-dispatcher"
    eos-commit-interval = 100ms
    kafka-clients {
      bootstrap.servers = "localhost:6001"
    }
  }

  kafka.consumer {
    bootstrap.servers = "localhost:6001"
    client.id = "akka-streams-kafka-client-id"
    group.id = "akka-streams-kafka-group-id"
    enable.auto.commit = false
    auto.offset.reset = "earliest"
    auto.commit.interval.ms = 100
    session.timeout.ms = 30000
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    poll-interval = 50ms
    poll-timeout = 50ms
    stop-timeout = 30s
    close-timeout = 20s
    commit-timeout = 15s
    commit-time-warning = 1s
    commit-refresh-interval = infinite
    use-dispatcher = "akka.kafka.default-dispatcher"
    kafka-clients {
      enable.auto.commit = false
      bootstrap.servers = "localhost:6001"
      group.id = "akka-streams-kafka-group-id"
      auto.offset.reset = "earliest"
    }
    wait-close-partition = 500ms
    position-timeout = 5s
    offset-for-times-timeout = 5s
    metadata-request-timeout = 5s
    eos-draining-check-interval = 30ms
  }

  kafka.committer {
    max-batch = 100
    max-interval = 10s
    parallelism = 4
  }
}